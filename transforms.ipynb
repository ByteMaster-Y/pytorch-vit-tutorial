{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FVvhLWUD_zHd"
      },
      "outputs": [],
      "source": [
        "# 데이터셋을 DataLoader로 변환하고, 모델을 정의하며, 학습 및 평가 루프를 직접 구현하는 과정\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "# 파이토치 기본 공식 베이직 코드를 참조함\n",
        "# ds = datasets.FashionMNIST(\n",
        "#     root=\"data\",\n",
        "#     train=True,\n",
        "#     download=True,\n",
        "#     transform=ToTensor(),\n",
        "#     target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        "# )\n",
        "\n",
        "# 원-핫 인코딩을 하는 과정\n",
        "# '셔츠'를 나타내는 레이블 y = 6이 이 함수에 전달될 때 가정,\n",
        "# torch.zeros(10, ...)이 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.] 텐서를 생성.\n",
        "# .scatter_ 메소드가 이 텐서를 받습니다. -> _의미: 제자리에서 작업이 이루어진다는 뜻\n",
        "# torch.tensor(y)가 torch.tensor(6)이 되므로, 인덱스 6을 사용.\n",
        "# 인덱스 6의 위치에 value=1을 넣습니다.\n",
        "# 그 결과, [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]와 같은 텐서가 완성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 준비\n",
        "training_data = datasets.FashionMNIST(\n",
        "  root=\"data\",\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=ToTensor(),\n",
        "  target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "  root=\"data\",\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=ToTensor(),\n",
        "  target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ],
      "metadata": {
        "id": "kx_70HK4CX2R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader 생성\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "aD5tVDY2Cr_6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transformer (VIT)모델 정의\n",
        "# ViT는 이미지를 패치로 분할한 후, 각 패치를 시퀀스로 다룹니다.\n",
        "# cnn은 필터를 이동시켜 이미지의 지역적 특징을 추출하는 반면에 ViT는 이미지들을 작은 크기의 여러 패치로 나눈 후, 각 패치를 마치 문장의 단어처럼 다룹니다.\n",
        "# 이 패치들의 순서 시퀀스를 트랜스포머 모델에 입력하여 전체 이미지의 특징을 학습합니다.\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "  def __init__(self, in_channels=1, patch_size=4, embed_dim=128, num_layers=2, num_heads=4, num_classes=10):\n",
        "    super().__init__()\n",
        "\n",
        "    image_size = 28\n",
        "\n",
        "    # 1. 패치 임베딩: 이미지를 패치로 나누고 각 패치를 벡터로 변환\n",
        "    # 28x28 이미지를 4x4 크기의 겹치지 않는(stride=4) 조각들로 나눕니다. 이렇게 만들어진 각 패치는 embed_dim (128) 차원의 벡터로 변환.\n",
        "    self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    # 2. 위치 인코딩: 패치들의 순서 정보를 추가\n",
        "    # num_patches + 1은 패치 개수(28/4 * 28/4 = 49)에 특별한 [CLS] 토큰 한 개를 더한 것입니다.\n",
        "    num_patches = (image_size // patch_size) * (image_size // patch_size)\n",
        "    self.positions = nn.Parameter(torch.randn(num_patches + 1, embed_dim))\n",
        "\n",
        "    # 3.[CLS] 토큰: 시퀀스 전체를 대표하는 특별한 토큰\n",
        "    # 트랜스포머 인코더는 이 토큰을 통해 전체 패치들의 정보를 한곳에 모읍니다. 학습이 끝나면 이 [CLS] 토큰의 출력이 전체 이미지를 대표하는 벡터가 됩니다. 우리는 이 벡터를 분류에 사용\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "    # 4. 트랜스포머 인코더: 패치 간의 관계를 학습\n",
        "    # nhead=num_heads 어텐션 헤드의 수 -> 어텐션 메커니즘이 한 번에 여러 관점에서 패치 간의 관계를 탐색\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    # 5. 분류 헤드: 최종 예측을 수행합니다.\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 이미지를 패치로 변환 (N, C, H, W) -> (N, P, E)\n",
        "    # flatten(2): 패치들을 1차원 시퀀스로 펼칩니다.\n",
        "    # transpose(1, 2):차원의 순서를 바꿉니다. 트랜스포머 입력에 맞는 (배치_크기, 패치_개수, 임베딩_차원) 형태로 만듭니다.\n",
        "    x = self.patch_embed(x)\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "    # [CLS] 토큰을 패치 시퀀스에 추가\n",
        "    cls_tokens = self.cls_token.expand(x.shape[0], -1, -1) # 0은 미니배치 데이터, -1은 해당 차원의 크기를 그대로\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    # 위치 임베딩 추가\n",
        "    x += self.positions\n",
        "\n",
        "    # 트랜스포머 인코더에 통과\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    # [CLS] 토큰의 출력을 사용하여 분류\n",
        "    cls_token_output = x[:, 0]\n",
        "    logits = self.classifier(cls_token_output)\n",
        "    return logits\n",
        "\n",
        "# 토큰은 시퀀스의 가장 앞에 배치되지만, 그 자체로는 어떤 정보도 가지고 있지 않습니다. 이 토큰의 역할은 오직 다른 모든 패치(이미지 조각)들로부터 정보를 모으고 요약하는 것입니다.\n",
        "# 트랜스포머의 핵심인 어텐션(Self-Attention) 메커니즘을 통해 이 작업이 이루어집니다.\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = SimpleViT()\n",
        "\n",
        "# 이미지 -> (패치로 분할) -> [CLS] 토큰과 위치 정보가 추가된 패치 시퀀스 -> 트랜스포머 인코더에 통과 -> [CLS] 토큰의 출력을 사용 -> 분류기에서 최종 예측."
      ],
      "metadata": {
        "id": "sV4xLo8qBfFv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수와 최적화기 정의 (이전과 동일)\n",
        "# 손실 함수(Loss Function): 모델의 예측과 실제 레이블 간의 차이를 계산합니다.\n",
        "# 최적화기(Optimizer): 손실 함수 값을 줄이기 위해 모델의 파라미터(가중치)를 업데이트합니다.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 학습 루프 정의\n",
        "# 이 함수는 DataLoader를 순회하며 모델을 학습시킵니다.\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train() # 모델을 학습 모드로 설정\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # 예측 및 손실 계산\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # 역전파(Backpropagation)\n",
        "        optimizer.zero_grad() # 이전 기울기 초기화\n",
        "        loss.backward()       # 손실에 대한 기울기 계산\n",
        "        optimizer.step()      # 가중치 업데이트\n",
        "\n",
        "        # 학습 진행 상황 출력\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# 테스트 루프 정의\n",
        "# 학습된 모델의 성능을 평가합니다.\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval() # 모델을 평가 모드로 설정\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad(): # 기울기 계산을 비활성화하여 메모리를 절약하고 속도를 높입니다.\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # 이 한 줄은 \"모델이 예측한 클래스 인덱스\"와 \"실제 정답의 클래스 인덱스\"를 비교하여, 예측이 일치하는 샘플의 개수를 세는 코드입니다.\n",
        "            # 이 개수를 전체 샘플 수로 나누면 정확도(Accuracy)를 계산할 수 있습니다.\n",
        "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# 학습 실행\n",
        "# 지정된 epoch 수만큼 학습과 평가를 반복합니다.\n",
        "epochs = 2\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n"
      ],
      "metadata": {
        "id": "ZA55qI2-G8Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b0cc4e-dd9a-4a8c-8372-df3f8886fa5e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.505097  [   64/60000]\n",
            "loss: 1.009578  [ 6464/60000]\n",
            "loss: 0.955084  [12864/60000]\n",
            "loss: 0.670252  [19264/60000]\n",
            "loss: 0.761983  [25664/60000]\n",
            "loss: 0.715345  [32064/60000]\n",
            "loss: 0.652899  [38464/60000]\n",
            "loss: 0.705130  [44864/60000]\n",
            "loss: 0.537867  [51264/60000]\n",
            "loss: 0.563349  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 0.559000 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.624593  [   64/60000]\n",
            "loss: 0.747637  [ 6464/60000]\n",
            "loss: 0.731733  [12864/60000]\n",
            "loss: 0.461139  [19264/60000]\n",
            "loss: 0.630891  [25664/60000]\n",
            "loss: 0.727792  [32064/60000]\n",
            "loss: 0.619790  [38464/60000]\n",
            "loss: 0.596877  [44864/60000]\n",
            "loss: 0.361807  [51264/60000]\n",
            "loss: 0.594487  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.517823 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}
