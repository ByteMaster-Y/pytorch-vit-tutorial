# Vision Transformer (ViT) with FashionMNIST

이 프로젝트는 PyTorch를 사용하여 **Vision Transformer (ViT)** 모델을 구현하고,  
FashionMNIST 데이터셋을 학습 및 평가하는 과정을 보여줍니다.  

---

## 학습 알고리즘 개요
이 코드는 총 **8단계**를 거쳐 모델을 학습하고 평가합니다.

### 1. 라이브러리 불러오기
- PyTorch 핵심 모듈(`torch`, `nn`, `DataLoader`)을 불러와 모델 학습을 준비합니다.

### 2. 데이터셋 준비
- FashionMNIST 데이터셋 다운로드
- 입력 이미지(X): 텐서로 변환
- 레이블(y): 원-핫 인코딩 적용

### 3. 데이터로더 생성
- `DataLoader`를 이용해 학습/테스트 데이터셋을 **mini-batch 단위**로 불러올 수 있도록 준비합니다.

### 4. 모델 정의: `SimpleViT`
`SimpleViT` 클래스는 다음 5가지 주요 부분으로 구성됩니다:

1. **패치 임베딩 (Patch Embedding)**: 이미지를 작은 패치로 나누고 벡터로 변환  
2. **위치 인코딩 (Positional Encoding)**: 패치들의 순서를 학습할 수 있도록 위치 정보 추가  
3. **[CLS] 토큰**: 전체 이미지를 대표하는 특별한 토큰  
4. **트랜스포머 인코더 (Transformer Encoder)**: 패치들 간의 관계 학습  
5. **분류 헤드 (Classification Head)**: 최종적으로 이미지를 분류

### 5. 손실 함수 및 최적화기
- **손실 함수**: `CrossEntropyLoss`  
- **최적화기**: `Adam` (손실을 최소화하도록 가중치 업데이트)

### 6. 학습 루프 실행 (`train_loop`)
- 각 에폭(epoch)마다 다음 과정을 반복:
  1. 모델이 입력 이미지를 예측 (`pred = model(X)`)  
  2. 손실(loss) 계산  
  3. 기울기(gradient) 역전파  
  4. 가중치 업데이트  

### 7. 평가 루프 실행 (`test_loop`)
- 각 에폭 종료 시, 테스트 데이터셋을 이용하여:
  - 손실과 정확도를 계산
  - **모델의 성능 평가** (가중치 업데이트는 없음)

### 8. 학습 완료
- 모든 에폭이 끝나면 `학습 완료!` 메시지 출력 후 종료

---

## 핵심 개념 도식화
ViT가 이미지를 처리하고 분류하는 과정을 시각적으로 요약하면 다음과 같습니다:

1. **입력 이미지 (Input Image)**  
   - FashionMNIST의 28×28 흑백 이미지 사용  

2. **패치 임베딩 (Patch Embedding)**  
   - 이미지를 4×4 패치로 분할  
   - 각 패치를 128차원 벡터로 변환  

3. **[CLS] 토큰 & 위치 인코딩 (Positional Encoding)**  
   - [CLS] 토큰은 전체 이미지 정보를 요약  
   - 패치 순서 정보를 보존하기 위해 위치 인코딩 추가  

4. **트랜스포머 인코더 (Transformer Encoder)**  
   - 패치 간 관계를 학습하고 [CLS] 토큰에 정보를 집약  

5. **분류기 (Classifier)**  
   - [CLS] 토큰 벡터를 이용해 최종적으로 **10개 클래스(옷, 신발 등)** 중 하나로 분류  

---

## 정리
이 프로젝트는 CNN 대신 트랜스포머 구조를 사용해 **이미지를 분류**하는 과정을 다룹니다.  
특히 ViT의 핵심 아이디어인 **패치 임베딩 → [CLS] 토큰 → 트랜스포머 인코더 → 분류기** 흐름을 이해하는 데 중점을 둡니다.
